{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [spaCy](http://spacy.io/docs/#examples) introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spaCy resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import spacy and English models\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading spaCy can take a while, in the meantime here are a few definitions to help you on your NLP journey.\n",
    "\n",
    "#### What are Stop Words?\n",
    "\n",
    "Stop words are the common words in a vocabulary which are of little value when considering word frequencies in text. This is because they don't provide much useful information about what the sentence is telling the reader.\n",
    "\n",
    "Example: _\"the\",\"and\",\"a\",\"are\",\"is\"_\n",
    "\n",
    "#### What is a Corpus?\n",
    "\n",
    "A corpus (plural: corpora) is a large collection of text or documents and can provide useful training data for NLP models. A corpus might be built from transcribed speech or a collection of manuscripts. Each item in a corpus is not necessarily unique and frequency counts of words can assist in uncovering the structure in a corpus.\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. Every word written in the complete works of Shakespeare\n",
    "2. Every word spoken on BBC Radio channels for the past 30 years "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process sentences 'Hello, world. Natural Language Processing in 10 lines of code.' using spaCy\n",
    "doc = nlp(u'Hello, world. Natural Language Processing in 10 lines of code.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tokens and sentences\n",
    "\n",
    "#### What is a Token?\n",
    "A token is a single chopped up element of the sentence, which could be a word or a group of words to analyse. The task of chopping the sentence up is called \"tokenisation\".\n",
    "\n",
    "Example: The following sentence can be tokenised by splitting up the sentence into individual words.\n",
    "\n",
    "\t\"Cytora is going to PyCon!\"\n",
    "\t[\"Cytora\",\"is\",\"going\",\"to\",\"PyCon!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hello, world.\n",
      "Natural Language Processing in 10 lines of code.\n"
     ]
    }
   ],
   "source": [
    "# Get first token of the processed document\n",
    "token = doc[0]\n",
    "print(token)\n",
    "\n",
    "# Print sentences (one sentence per line)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tags\n",
    "\n",
    "#### What is a Speech Tag?\n",
    "A speech tag is a context sensitive description of what a word means in the context of the whole sentence.\n",
    "More information about the kinds of speech tags which are used in NLP can be [found here](http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/).\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. CARDINAL, Cardinal Number - 1,2,3\n",
    "2. PROPN, Proper Noun, Singular - \"Matic\", \"Andraz\", \"Cardiff\"\n",
    "3. INTJ, Interjection - \"Uhhhhhhhhhhh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello - INTJ\n",
      ", - PUNCT\n",
      "world - NOUN\n",
      ". - PUNCT\n",
      "Natural - PROPN\n",
      "Language - PROPN\n",
      "Processing - PROPN\n",
      "in - ADP\n",
      "10 - NUM\n",
      "lines - NOUN\n",
      "of - ADP\n",
      "code - NOUN\n",
      ". - PUNCT\n"
     ]
    }
   ],
   "source": [
    "# For each token, print corresponding part of speech tag\n",
    "for token in doc:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual part of speech tagging ([displaCy](https://displacy.spacy.io))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic dependencies\n",
    "\n",
    "#### What are syntactic dependencies?\n",
    "\n",
    "We have the speech tags and we have all of the tokens in a sentence, but how do we relate the two to uncover the syntax in a sentence? Syntactic dependencies describe how each type of word relates to each other in a sentence, this is important in NLP in order to extract structure and understand grammar in plain text.\n",
    "\n",
    "Example:\n",
    "\n",
    "<img src=\"images/syntax-dependencies-oliver.png\" align=\"left\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello --> []\n",
      ", --> [,, Hello]\n",
      "world --> [world, Hello]\n",
      ". --> [., Hello]\n",
      "Natural --> [Natural, Processing]\n",
      "Language --> [Language, Processing]\n",
      "Processing --> []\n",
      "in --> [in, Processing]\n",
      "10 --> [10, lines, lines, in, in, Processing]\n",
      "lines --> [lines, in, in, Processing]\n",
      "of --> [of, lines, lines, in, in, Processing]\n",
      "code --> [code, of, of, lines, lines, in, in, Processing]\n",
      ". --> [., Processing]\n",
      "\n",
      ",-punct-> Hello-ROOT\n",
      "world-npadvmod-> Hello-ROOT\n",
      ".-punct-> Hello-ROOT\n",
      "Natural-compound-> Processing-ROOT\n",
      "Language-compound-> Processing-ROOT\n",
      "\n",
      "in-prep-> Processing-ROOT\n",
      "10-nummod-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "of-prep-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      "code-pobj-> of-prep-> of-prep-> lines-pobj-> lines-pobj-> in-prep-> in-prep-> Processing-ROOT\n",
      ".-punct-> Processing-ROOT\n"
     ]
    }
   ],
   "source": [
    "# Write a function that walks up the syntactic tree of the given token and collects all tokens to the root token (including root token).\n",
    "\n",
    "def tokens_to_root(token):\n",
    "    \"\"\"\n",
    "    Walk up the syntactic tree, collecting tokens to the root of the given `token`.\n",
    "    :param token: Spacy token\n",
    "    :return: list of Spacy tokens\n",
    "    \"\"\"\n",
    "    tokens_to_r = []\n",
    "    while token.head is not token:\n",
    "        tokens_to_r.append(token)\n",
    "        token = token.head\n",
    "        tokens_to_r.append(token)\n",
    "\n",
    "    return tokens_to_r\n",
    "\n",
    "# For every token in document, print it's tokens to the root\n",
    "for token in doc:\n",
    "    print('{} --> {}'.format(token, tokens_to_root(token)))\n",
    "\n",
    "# Print dependency labels of the tokens\n",
    "for token in doc:\n",
    "    print('-> '.join(['{}-{}'.format(dependent_token, dependent_token.dep_) for dependent_token in tokens_to_root(token)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entities\n",
    "\n",
    "#### Named Entities\n",
    "\n",
    "A named entity is any real world object such as a person, location, organisation or product with a proper name. \n",
    "\n",
    "Example:\n",
    "\n",
    "\t1. Barack Obama\n",
    "\t2. Edinburgh\n",
    "\t3. Ferrari Enzo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris - GPE\n",
      "Jack - PERSON\n"
     ]
    }
   ],
   "source": [
    "# Print all named entities with named entity types\n",
    "\n",
    "doc_2 = nlp(u\"I went to Paris where I met my old friend Jack from uni.\")\n",
    "for ent in doc_2.ents:\n",
    "    print('{} - {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun chunks\n",
    "\n",
    "#### What is a Noun Chunk?\n",
    "Noun chunks are the phrases based upon nouns recovered from tokenized text using the speech tags.\n",
    "\n",
    "Example:\n",
    "\n",
    "The sentence \"The boy saw the yellow dog\" has 2 noun objects, the boy and the dog. \n",
    "Therefore the noun chunks will be\n",
    "\n",
    "\t1. \"The boy\"\n",
    "\t2. \"the yellow dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, Paris, I, my old friend, uni]\n"
     ]
    }
   ],
   "source": [
    "# Print noun chunks for doc_2\n",
    "print([chunk for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[I, <class 'spacy.tokens.span.Span'>], [Paris, <class 'spacy.tokens.span.Span'>], [I, <class 'spacy.tokens.span.Span'>], [my old friend, <class 'spacy.tokens.span.Span'>], [uni, <class 'spacy.tokens.span.Span'>]]\n"
     ]
    }
   ],
   "source": [
    "# Note that chunk is not string\n",
    "print([[chunk,type(chunk)] for chunk in doc_2.noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my old friend', 'uni', 'Paris', 'I'}\n"
     ]
    }
   ],
   "source": [
    "# Print noun chunks for doc_2\n",
    "print(set([str(chunk) for chunk in doc_2.noun_chunks]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For every token in doc_2, print log-probability of the word, estimated from counts from a large corpus \n",
    "tok_dic = {}\n",
    "for token in doc_2:\n",
    "    #print(token, ',', token.prob)\n",
    "    tok_dic[str(token)]=token.prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': -3.0729479789733887,\n",
       " 'I': -4.064180850982666,\n",
       " 'Jack': -11.20296573638916,\n",
       " 'Paris': -11.6917724609375,\n",
       " 'friend': -8.825821876525879,\n",
       " 'from': -6.028810501098633,\n",
       " 'met': -9.784490585327148,\n",
       " 'my': -5.918124675750732,\n",
       " 'old': -7.7954816818237305,\n",
       " 'to': -3.83851957321167,\n",
       " 'uni': -19.579313278198242,\n",
       " 'went': -8.474893569946289,\n",
       " 'where': -7.183883190155029}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import mylib\n",
    "import sys\n",
    "if not 'zapme' in ';'.join(sys.path):\n",
    "    sys.path.insert(0,'D:\\\\zapme\\\\Dropbox\\\\docs\\\\codes\\\\python\\\\github')\n",
    "\n",
    "import wgonglib as wg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-3.0729479789733887, '.'),\n",
       " (-3.83851957321167, 'to'),\n",
       " (-4.064180850982666, 'I'),\n",
       " (-5.918124675750732, 'my'),\n",
       " (-6.028810501098633, 'from'),\n",
       " (-7.183883190155029, 'where'),\n",
       " (-7.7954816818237305, 'old'),\n",
       " (-8.474893569946289, 'went'),\n",
       " (-8.825821876525879, 'friend'),\n",
       " (-9.784490585327148, 'met'),\n",
       " (-11.20296573638916, 'Jack'),\n",
       " (-11.6917724609375, 'Paris'),\n",
       " (-19.579313278198242, 'uni')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg.sort_dict_by_val(tok_dic, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', -3.0729479789733887),\n",
       " ('I', -4.064180850982666),\n",
       " ('Jack', -11.20296573638916),\n",
       " ('Paris', -11.6917724609375),\n",
       " ('friend', -8.825821876525879),\n",
       " ('from', -6.028810501098633),\n",
       " ('met', -9.784490585327148),\n",
       " ('my', -5.918124675750732),\n",
       " ('old', -7.7954816818237305),\n",
       " ('to', -3.83851957321167),\n",
       " ('uni', -19.579313278198242),\n",
       " ('went', -8.474893569946289),\n",
       " ('where', -7.183883190155029)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg.sort_dict_by_key(tok_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding / Similarity\n",
    "\n",
    "#### What are Word embeddings?\n",
    "\n",
    "A word embedding is a representation of a word, and by extension a whole language corpus, in a vector or other form of numerical mapping. This allows words to be treated numerically with word similarity represented as spatial difference in the dimensions of the word embedding mapping.\n",
    "\n",
    "Example:\n",
    "\t\n",
    "With word embeddings we can understand that vector operations describe word similarity. This means that we can see vector proofs of statements such as:\n",
    "\n",
    "\tking-queen==man-woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# For a given document, calculate similarity \n",
    "# between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print(apples.similarity(oranges))\n",
    "print(boots.similarity(hippos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.569403101179\n",
      "0.323890751106\n"
     ]
    }
   ],
   "source": [
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print(apples_sent.similarity(fruit))\n",
    "print(boots_sent.similarity(fruit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
